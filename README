# ğŸ§  Zetalbot: Local LLM Fine-Tuning + RAG Assistant

This project enables a fully local, GPU-accelerated AI assistant based on **LLaMA 3.1 8B**, with support for:
- âœ… LoRA fine-tuning (`peft`)
- âœ… Quantized inference (4-bit via `BitsAndBytes`)
- âœ… Retrieval-Augmented Generation (RAG) using `ChromaDB`
- âœ… Persistent chat with structured history

---

## ğŸ“‚ Folder Structure
.
â”œâ”€â”€ dataset_json/ # JSON/JSONL files for LoRA fine-tuning
â”œâ”€â”€ rag_documents/ # Documents to index with RAG (ChromaDB)
â”œâ”€â”€ output_lora/ # Directory where LoRA checkpoints are saved (It is created via script. It can be deleted to clean the data and recreated.)
â”œâ”€â”€ chromadb/ # Auto-generated persistent vector store (It is created via script. It can be deleted to clean the data and recreated.)
â”œâ”€â”€ history_backup.json # JSON chat history (persisted) (It is created via script. It can be deleted to clean the data and recreated.)
â””â”€â”€ script.py # Main executable script

---

## âš™ï¸ Requirements

Python 3.9+ and the following libraries:

LIBRARY
torch transformers datasets peft sentence-transformers chromadb scikit-learn
USE THE REQUIREMENTS FILE TO IMPORT THE LIBRARY (I have the file with all the libraries that i used. Many are not necessary for the project. You can use only the ones required and remove the others.)
âš ï¸ Requires an NVIDIA GPU with support for:

Mixed precision (float16)

At least 24GB VRAM for LLaMA 3.1 8B with 4-bit quantization

ğŸš€ Quick Start
bash
Copia
Modifica
python3 script.py
ğŸ¤– Available Commands (CLI):
start training â†’ Launch LoRA fine-tuning

start rag â†’ Index documents from rag_documents/

exit â†’ Close chat session

ğŸ› ï¸ Configuration Overview
Setting	Description
BASE_MODEL_PATH	Hugging Face model, e.g. meta-llama/Llama-3.1-8B-Instruct
TRAINED_LORA_PATH	Path to saved LoRA checkpoint
MODEL_MODE	"base" or "trained"
SYSTEM_PROMPT	System instruction at start of chat
MAX_CONTEXT_TOKENS	Max tokens in prompt (context)
MAX_NEW_TOKENS	Max tokens generated per answer

ğŸ“š Dataset Format (LoRA Training)
Each record should follow this schema:
{
  "instruction": "Translate to French",
  "input": "Hello world",
  "response": "Bonjour le monde"
}
Place one or more .json or .jsonl files in dataset_json/.

ğŸ“„ RAG Document Format
Each indexed document must include:
{
  "id": "doc1",
  "content": "This is the full text to embed and retrieve.",
  "metadata": {"source": "manual", "topic": "setup"}
}
Add documents to rag_documents/, then run start rag.

ğŸ§ª LoRA Fine-Tuning Details
Uses:

AutoModelForCausalLM + BitsAndBytesConfig

prepare_model_for_kbit_training()

Target modules: q_proj, k_proj, v_proj, o_proj

Trainer from ğŸ¤— Transformers

Training config:
python
Copia
Modifica
TrainingArguments(
  per_device_train_batch_size=1,
  gradient_accumulation_steps=8,
  learning_rate=2e-4,
  num_train_epochs=3,
  fp16=True
)
ğŸ” Retrieval-Augmented Generation (RAG)
Embeddings from: sentence-transformers/msmarco-distilbert-base-v3

Stored in: ChromaDB (persistent)

Used at inference time to enrich prompts with real-world context

ğŸ’¬ Chat History
The chat uses a JSON file (history_backup.json) to:

Save and load history

Truncate oldest messages when token limit is exceeded

System, user, and assistant messages are preserved across sessions.

âœ… Features Summary
 Offline operation

 Train custom assistant locally

 Extend context via document retrieval (RAG)

 Token-aware chat truncation

 GPU quantized model loading (4bit)

ğŸ§° Recommended Hardware
Component	Minimum
GPU	NVIDIA with 24GB VRAM
RAM	16 GB
Disk	30 GB+ (model, cache, datasets)

ğŸ“„ License
MIT License â€” use at your own risk. This project is intended for local, private experimentation only. Ensure you comply with licenses of any third-party model (e.g. Meta's LLaMA).

ğŸ™‹â€â™‚ï¸ Author
Zetalbot is maintained by Zetalvx.
Created for research, experimentation, and learning about local LLMs.
Youtube Channel: https://youtube.com/@zetalvx?si=76ovE6TxFKxUwB6e
